{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89PuGl57t6hB",
    "outputId": "a858a74b-fceb-48b5-f759-34cdc1766b43"
   },
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YkGCv91Ct-Xw"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/Documents/3rd year/AI/Assignment/datasets/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(preprocessed_train_data_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     90\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(preprocessed_validation_data_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 92\u001b[0m \u001b[43mpreprocessImages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_train_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessed_train_data_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m preprocessImages(raw_validation_data_dir, preprocessed_validation_data_dir)\n",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m, in \u001b[0;36mpreprocessImages\u001b[1;34m(raw_data_dir, preprocessed_data_dir)\u001b[0m\n\u001b[0;32m     29\u001b[0m num_augmented_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Iterate over the raw images in the directory\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     33\u001b[0m   image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(raw_data_dir, image_file)\n\u001b[0;32m     35\u001b[0m   \u001b[38;5;28mprint\u001b[39m(image_file)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/Documents/3rd year/AI/Assignment/datasets/train'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from cv2 import CascadeClassifier\n",
    "import os\n",
    "from cv2 import imread\n",
    "from cv2 import waitKey\n",
    "from cv2 import destroyAllWindows\n",
    "from cv2 import CascadeClassifier\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def preprocessImages(raw_data_dir, preprocessed_data_dir):\n",
    "\n",
    "  datagen = ImageDataGenerator(\n",
    "      rotation_range=30,\n",
    "      width_shift_range=0.1,\n",
    "      height_shift_range=0.1,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      )\n",
    "\n",
    "  # Define the target size for resizing\n",
    "  target_size = (128, 128)\n",
    "\n",
    "  # Define the number of augmented images to generate\n",
    "  num_augmented_images = 100\n",
    "\n",
    "  # Iterate over the raw images in the directory\n",
    "  for image_file in os.listdir(raw_data_dir):\n",
    "    image_path = os.path.join(raw_data_dir, image_file)\n",
    "\n",
    "    print(image_file)\n",
    "    for imageName in os.listdir(image_path):\n",
    "\n",
    "      # Load the image\n",
    "      image_dir = os.path.join(image_path, imageName)\n",
    "      image = cv2.imread(image_dir)\n",
    "\n",
    "      # Resize the image\n",
    "      resized_image = cv2.resize(image, target_size)\n",
    "\n",
    "      temp = 0\n",
    "      input_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "      input_image = np.array(input_image)\n",
    "      input_image = input_image.reshape((1,) + input_image.shape)  # Reshape for single image\n",
    "\n",
    "      # Apply data augmentation to generate additional images\n",
    "      for i in range(num_augmented_images):\n",
    "        temp +=1\n",
    "        if temp == num_augmented_images:\n",
    "          break\n",
    "\n",
    "        augmented_images = datagen.flow(input_image, batch_size=1, save_format='jpg')\n",
    "        augmented_image = next(augmented_images)[0]\n",
    "        augmented_image = augmented_image.astype(np.uint8)\n",
    "\n",
    "          # Perform face detection and crop the image to the face region\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(augmented_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        # Iterate over the detected faces\n",
    "        for (x, y, w, h) in faces:\n",
    "            # store_dir = os.path.join(preprocessed_data_dir, image_file, imageName)\n",
    "            # new_dir = os.path.join(store_dir, f'face_{image_file.split(\".\")[0]}')\n",
    "            store_dir = os.path.join(preprocessed_data_dir, image_file)\n",
    "            print(store_dir)\n",
    "            os.makedirs(store_dir, exist_ok=True)\n",
    "            # Crop the image to the face region\n",
    "            cropped_image = augmented_image[y:y+h, x:x+w]\n",
    "\n",
    "            # Save the augmented image with a unique filename\n",
    "            augmented_image_filename = f'{imageName.split(\".\")[0]}_{i}.jpg'\n",
    "            augmented_image_path = os.path.join(store_dir, augmented_image_filename)\n",
    "            cv2.imwrite(augmented_image_path, cropped_image)\n",
    "\n",
    "# Define the directory containing the raw images\n",
    "# raw_data_dir = '/content/drive/MyDrive/Colab Notebooks/Assignment2/train'\n",
    "raw_train_data_dir = 'D:\\Documents\\3rd year\\AI\\Assignment 2\\myAssignment\\datasets\\train'\n",
    "raw_validation_data_dir = 'D:\\Documents\\3rd year\\AI\\Assignment 2\\myAssignment\\datasets\\validation'\n",
    "\n",
    "# Define the output directory for the preprocessed dataset\n",
    "preprocessed_train_data_dir = '/content/sample_data/preprocessed_dataset/train'\n",
    "preprocessed_validation_data_dir = '/content/sample_data/preprocessed_dataset/validation'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(preprocessed_train_data_dir, exist_ok=True)\n",
    "os.makedirs(preprocessed_validation_data_dir, exist_ok=True)\n",
    "\n",
    "preprocessImages(raw_train_data_dir, preprocessed_train_data_dir)\n",
    "preprocessImages(raw_validation_data_dir, preprocessed_validation_data_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNhNqbVUwFU5",
    "outputId": "ff56f605-3ac6-44cb-b014-88dcab8f429a"
   },
   "outputs": [],
   "source": [
    "# Deep Learning CNN model to recognize face\n",
    "'''This script uses a database of images and creates CNN model on top of it to test\n",
    "   if the given image is recognized correctly or not'''\n",
    "\n",
    "'''####### IMAGE PRE-PROCESSING for TRAINING and TESTING data #######'''\n",
    "\n",
    "# Specifying the folder where images are present\n",
    "\n",
    "\n",
    "\n",
    "# Understand more about ImageDataGenerator at below link\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "# Defining pre-processing transformations on raw images of training data\n",
    "# These hyper parameters helps to generate slightly twisted versions\n",
    "# of the original image, which leads to a better model, since it learns\n",
    "# on the good and bad mix of images\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#         shear_range=0.1,\n",
    "#         zoom_range=0.1,\n",
    "#         horizontal_flip=True,\n",
    "#         rotation_range=20,\n",
    "#         width_shift_range=0.1,\n",
    "#         height_shift_range=0.1,\n",
    "#          fill_mode='nearest')\n",
    "\n",
    "# # Defining pre-processing transformations on raw images of testing data\n",
    "# # No transformations are done on the testing images\n",
    "TrainingImagePath = '/content/sample_data/preprocessed_dataset/train'\n",
    "validationImagePath = '/content/sample_data/preprocessed_dataset/validation'\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# # Generating the Training Data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "# Generating the Testing Data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        validationImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Printing class labels for each face\n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99eYkBsWwg8V",
    "outputId": "77fde07b-0773-4541-c316-99df472ff69b"
   },
   "outputs": [],
   "source": [
    "'''############ Creating lookup table for all faces ############'''\n",
    "# class_indices have the numeric tag for each face\n",
    "TrainClasses=training_set.class_indices\n",
    "\n",
    "# Storing the face and the numeric tag for future reference\n",
    "ResultMap={}\n",
    "for faceValue,faceName in zip(TrainClasses.values(),TrainClasses.keys()):\n",
    "    ResultMap[faceValue]=faceName\n",
    "\n",
    "# Saving the face map for future reference\n",
    "import pickle\n",
    "with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
    "    pickle.dump(ResultMap, fileWriteStream)\n",
    "\n",
    "# The model will give answer as a numeric tag\n",
    "# This mapping will help to get the corresponding face name for it\n",
    "print(\"Mapping of Face and its ID\",ResultMap)\n",
    "\n",
    "# The number of neurons for the output layer is equal to the number of faces\n",
    "OutputNeurons=len(ResultMap)\n",
    "print('\\n The Number of output neurons: ', OutputNeurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ay9QR7SRwkXb",
    "outputId": "351573cd-135c-43cf-d1c1-4c38d6a2c7a5"
   },
   "outputs": [],
   "source": [
    "'''######################## Create CNN deep learning model ########################'''\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "\n",
    "'''Initializing the Convolutional Neural Network'''\n",
    "classifier= Sequential()\n",
    "\n",
    "''' STEP--1 Convolution\n",
    "# Adding the first layer of CNN\n",
    "# we are using the format (64,64,3) because we are using TensorFlow backend\n",
    "# It means 3 matrix of size (64X64) pixels representing Red, Green and Blue components of pixels\n",
    "'''\n",
    "classifier.add(Convolution2D(32, kernel_size=(3, 3), strides=(1, 1), input_shape=(64,64,3), activation='relu'))\n",
    "\n",
    "'''# STEP--2 MAX Pooling'''\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "'''############## ADDITIONAL LAYER of CONVOLUTION for better accuracy #################'''\n",
    "classifier.add(Convolution2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
    "\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Convolution2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
    "\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "'''# STEP--3 FLattening'''\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "'''# STEP--4 Fully Connected Neural Network'''\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "\n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
    "\n",
    "'''# Compiling the CNN'''\n",
    "#classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Starting the model training\n",
    "history = classifier.fit(\n",
    "                    training_set,\n",
    "                    epochs=7,\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1nBDJOiLsxo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZKvHcTeLus_"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  data = eval_js('takePhoto({})'.format(quality))\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)\n",
    "  return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxAoV-u1LutA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCgAMxdOLHf-",
    "outputId": "1e401fbc-f3a5-4867-d71f-adefc0e0f582"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.utils as image\n",
    "\n",
    "from IPython.display import Image\n",
    "try:\n",
    "  # filename = take_photo()\n",
    "  # print('Saved to {}'.format(filename))\n",
    "\n",
    "  # ImagePath='/content/' + filename\n",
    "  ImagePath = '/content/sample_data/preprocessed_dataset/validation/face6/face6_1_1.jpg'\n",
    "  test_image=image.load_img(ImagePath,target_size=(64, 64))\n",
    "  test_image=image.img_to_array(test_image)\n",
    "\n",
    "  test_image=np.expand_dims(test_image,axis=0)\n",
    "\n",
    "  result=classifier.predict(test_image,verbose=0)\n",
    "  # print(training_set.class_indices)\n",
    "\n",
    "  print('####'*10)\n",
    "  print('Prediction is: ',ResultMap[np.argmax(result)])\n",
    "\n",
    "  # Show the image which was just taken.\n",
    "  # display(Image(filename))\n",
    "except Exception as err:\n",
    "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "  # grant the page permission to access it.\n",
    "  print(str(err))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "id": "vyG2JkT24HMv",
    "outputId": "e8da1bac-1a18-4f71-c48d-38c47cf9eac2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1mWuE-nZXvH",
    "outputId": "96f7eeef-a7fe-418d-cd5c-aa9262f59d6b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x857vAOVZvsg",
    "outputId": "6190ad73-d45b-4a76-95d3-5684622ecdef"
   },
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "UsAR_JOcZ1Z_",
    "outputId": "c9d96005-3f45-41a3-888d-725f3e0d33b0"
   },
   "outputs": [],
   "source": [
    " ImagePath = '/content/drive/MyDrive/AI/datasets/validation/face6/face6_1.jpg'\n",
    "  test_image= cv2.imread(ImagePath)\n",
    "  test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "  test_image = cv2.resize(test_image, (64, 64))\n",
    "  # test_image=image.img_to_array(test_image)\n",
    "\n",
    "  test_image=np.expand_dims(test_image,axis=0)\n",
    "\n",
    "  result=classifier.predict(test_image)\n",
    "  # print(training_set.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# for face detection\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# resolution of the webcam\n",
    "screen_width = 1280       # try 640 if code fails\n",
    "screen_height = 720\n",
    "\n",
    "# default webcam\n",
    "stream = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # capture frame-by-frame\n",
    "    (grabbed, frame) = stream.read()\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # try to detect faces in the webcam\n",
    "    faces = face_cascade.detectMultiScale(rgb, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # for each faces found\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the face\n",
    "        color = (0, 255, 255) # in BGR\n",
    "        stroke = 5\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, stroke)\n",
    "\n",
    "    # show the frame\n",
    "    cv2.imshow(\"Image\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):    # Press q to break out\n",
    "        break                  # of the loop\n",
    "\n",
    "# cleanup\n",
    "stream.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained face detection model (e.g., Haar cascades or Dlib)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('path_to_face_cascade.xml')\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained CNN model for face recognition\n",
    "\n",
    "model = keras.models.load_model('path_to_trained_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Load a list of authorized individuals for attendance registration\n",
    "\n",
    "authorized_individuals = ['John', 'Jane', 'Michael']  # Replace with your own list\n",
    "\n",
    "\n",
    "\n",
    "# Function to perform face recognition on an input image\n",
    "\n",
    "def recognize_face(image):\n",
    "\n",
    "    # Convert the image to grayscale for face detection\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "\n",
    "    # Perform face detection\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "\n",
    "        # Extract the face region from the image\n",
    "\n",
    "        face = gray[y:y+h, x:x+w]\n",
    "\n",
    "        \n",
    "\n",
    "        # Preprocess the face image (resize, normalize, etc.)\n",
    "\n",
    "        face = cv2.resize(face, (64, 64))\n",
    "\n",
    "        face = face / 255.0\n",
    "\n",
    "        face = np.expand_dims(face, axis=0)\n",
    "\n",
    "        \n",
    "\n",
    "        # Perform face recognition by passing the face through the CNN model\n",
    "\n",
    "        prediction = model.predict(face)\n",
    "\n",
    "        \n",
    "\n",
    "        # Retrieve the predicted label (person's identity)\n",
    "\n",
    "        label_index = np.argmax(prediction)\n",
    "\n",
    "        label = authorized_individuals[label_index]\n",
    "\n",
    "        \n",
    "\n",
    "        # Draw a rectangle and label around the detected face\n",
    "\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    \n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "# Capture video from the webcam\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Read a frame from the video stream\n",
    "\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "\n",
    "\n",
    "    # Perform face recognition on the frame\n",
    "\n",
    "    output = recognize_face(frame)\n",
    "\n",
    "\n",
    "\n",
    "    # Display the output frame\n",
    "\n",
    "    cv2.imshow('Face Recognition', output)\n",
    "\n",
    "\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "\n",
    "video_capture.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflowNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Version: 2.12.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
